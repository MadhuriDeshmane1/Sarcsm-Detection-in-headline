{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6ba3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a68ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4928e3",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bad3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = [json.loads(line) for \n",
    "        line in open(\"Sarcasm_Headlines_Dataset.json\", 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc329bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame.from_dict(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a24f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_df.drop(['article_link'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d332b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d2f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   headline      100 non-null    object\n",
      " 1   is_sarcastic  100 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "637a75e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827d900",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6645d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data.headline,data.is_sarcastic,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5438de4",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a248e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating functions for preprocessing\n",
    "def remove_newlines(data):\n",
    "    formatted_text = data.replace(\"\\\\n\",' ').replace(\"\\t\",\" \")\n",
    "    return formatted_text\n",
    "\n",
    "def contraction_map(data):\n",
    "    fixed_text = contractions.fix(data)\n",
    "    return fixed_text\n",
    "\n",
    "def handle_accented(data):\n",
    "    fixed_text = unidecode(data)\n",
    "    return fixed_text\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.remove(\"no\")\n",
    "stopword_list.remove(\"not\")\n",
    "stopword_list.remove(\"nor\")\n",
    "def cleaning_text(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    clean_tokens = [ i.lower() for i in tokens if (i.lower() not in stopword_list) and (i not in punctuation) ]\n",
    "    clean_tokens = [ i for i in  clean_tokens if (len(i)>1) and i.isalpha()]\n",
    "    return clean_tokens\n",
    "\n",
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_tokens = []\n",
    "    for i in data:\n",
    "        lemmatized_word = lemmatizer.lemmatize(i)\n",
    "        final_tokens.append(lemmatized_word)\n",
    "    return \" \".join(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95dd1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = x_train.apply(remove_newlines)\n",
    "clean_test = x_test.apply(remove_newlines)\n",
    "\n",
    "clean_train = clean_train.apply(contraction_map)\n",
    "clean_test = clean_test.apply(contraction_map)\n",
    "\n",
    "clean_train = clean_train.apply(handle_accented)\n",
    "clean_test = clean_test.apply(handle_accented)\n",
    "\n",
    "clean_train = clean_train.apply(cleaning_text)\n",
    "clean_test = clean_test.apply(cleaning_text)\n",
    "\n",
    "clean_train = clean_train.apply(lemmatization)\n",
    "clean_test = clean_test.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f265d9",
   "metadata": {},
   "source": [
    "# word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989d3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word indexing\n",
    "max_words = 1000\n",
    "tk = Tokenizer(num_words=max_words,oov_token=\"##oov##\")\n",
    "train_sent_list = clean_train.to_list()\n",
    "test_sent_list = clean_test.to_list()\n",
    "tk.fit_on_texts(train_sent_list)\n",
    "x_train_seq = tk.texts_to_sequences(train_sent_list)\n",
    "x_test_seq = tk.texts_to_sequences(test_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7d4104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_words_per_sent = 500\n",
    "x_train_seq = pad_sequences(x_train_seq,padding='post',maxlen = max_words_per_sent,truncating='post')\n",
    "x_test_seq = pad_sequences(x_test_seq,padding='post',maxlen = max_words_per_sent,truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39d5c8",
   "metadata": {},
   "source": [
    "# Building simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e92271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Embedding,SimpleRNN,Bidirectional,LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50eb6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6890 - accuracy: 0.5470 - val_loss: 0.6017 - val_accuracy: 0.7570\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.6978 - accuracy: 0.5474 - val_loss: 0.6347 - val_accuracy: 0.7569\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.6890 - accuracy: 0.5479 - val_loss: 0.6633 - val_accuracy: 0.7323\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.6861 - accuracy: 0.5491 - val_loss: 0.6337 - val_accuracy: 0.7458\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embdding_dim = 50\n",
    "model.add(Embedding(input_dim = max_words+1,output_dim = embdding_dim,input_length = max_words_per_sent))\n",
    "model.add(SimpleRNN(units=64,return_sequences=True))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(x_train_seq,y_train,batch_size=252,epochs=10,validation_data=(x_test_seq,y_test),callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a36bc8",
   "metadata": {},
   "source": [
    "# Deep RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a9e9031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7075 - accuracy: 0.4634 - val_loss: 0.6240 - val_accuracy: 0.7474\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.6928 - accuracy: 0.5425 - val_loss: 0.6825 - val_accuracy: 0.7437\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.6934 - accuracy: 0.5385 - val_loss: 0.6530 - val_accuracy: 0.7481\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.6899 - accuracy: 0.5415 - val_loss: 0.6356 - val_accuracy: 0.7487\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embdding_dim = 50\n",
    "model.add(Embedding(input_dim = max_words+1,output_dim = embdding_dim,input_length = max_words_per_sent))\n",
    "model.add(SimpleRNN(units=64,return_sequences=True))\n",
    "model.add(SimpleRNN(units=32,return_sequences=True))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(x_train_seq,y_train,batch_size=252,epochs=10,validation_data=(x_test_seq,y_test),callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13279c",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d9ee43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6944 - accuracy: 0.4644 - val_loss: 0.6117 - val_accuracy: 0.7594\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.6942 - accuracy: 0.5466 - val_loss: 0.6487 - val_accuracy: 0.7582\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.6887 - accuracy: 0.5467 - val_loss: 0.6557 - val_accuracy: 0.7594\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.6888 - accuracy: 0.5470 - val_loss: 0.6516 - val_accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embdding_dim = 50\n",
    "model.add(Embedding(input_dim = max_words+1,output_dim = embdding_dim,input_length = max_words_per_sent))\n",
    "model.add(Bidirectional(SimpleRNN(units=64,return_sequences=True)))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(x_train_seq,y_train,batch_size=252,epochs=10,validation_data=(x_test_seq,y_test),callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd200d",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65fbcd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6934 - accuracy: 0.4542 - val_loss: 0.6929 - val_accuracy: 0.7530\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.6931 - accuracy: 0.5463 - val_loss: 0.6917 - val_accuracy: 0.7588\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.6929 - accuracy: 0.5473 - val_loss: 0.6902 - val_accuracy: 0.7600\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.6926 - accuracy: 0.5469 - val_loss: 0.6886 - val_accuracy: 0.7600\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.6923 - accuracy: 0.5468 - val_loss: 0.6867 - val_accuracy: 0.7600\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.6920 - accuracy: 0.5467 - val_loss: 0.6847 - val_accuracy: 0.7600\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.6917 - accuracy: 0.5467 - val_loss: 0.6825 - val_accuracy: 0.7600\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.6914 - accuracy: 0.5467 - val_loss: 0.6803 - val_accuracy: 0.7600\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.6910 - accuracy: 0.5467 - val_loss: 0.6779 - val_accuracy: 0.7600\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.6907 - accuracy: 0.5467 - val_loss: 0.6755 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embdding_dim = 50\n",
    "model.add(Embedding(input_dim = max_words+1,output_dim = embdding_dim,input_length = max_words_per_sent))\n",
    "model.add(LSTM(units=8,return_sequences=True))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(x_train_seq,y_train,batch_size=252,epochs=10,validation_data=(x_test_seq,y_test),callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dbacf",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d2fc7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.6926 - accuracy: 0.5457 - val_loss: 0.6860 - val_accuracy: 0.7592\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.6919 - accuracy: 0.5469 - val_loss: 0.6823 - val_accuracy: 0.7597\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.6914 - accuracy: 0.5471 - val_loss: 0.6786 - val_accuracy: 0.7598\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.6908 - accuracy: 0.5472 - val_loss: 0.6745 - val_accuracy: 0.7600\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.6903 - accuracy: 0.5471 - val_loss: 0.6703 - val_accuracy: 0.7600\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.6898 - accuracy: 0.5470 - val_loss: 0.6660 - val_accuracy: 0.7600\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.6894 - accuracy: 0.5469 - val_loss: 0.6617 - val_accuracy: 0.7600\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.6891 - accuracy: 0.5469 - val_loss: 0.6573 - val_accuracy: 0.7600\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.6888 - accuracy: 0.5469 - val_loss: 0.6530 - val_accuracy: 0.7600\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.6886 - accuracy: 0.5469 - val_loss: 0.6489 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embdding_dim = 50\n",
    "model.add(Embedding(input_dim = max_words+1,output_dim = embdding_dim,input_length = max_words_per_sent))\n",
    "model.add(Bidirectional(LSTM(units=8,return_sequences=True)))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(x_train_seq,y_train,batch_size=252,epochs=10,validation_data=(x_test_seq,y_test),callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b3f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
